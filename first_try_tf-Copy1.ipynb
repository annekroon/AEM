{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install embedding vectorizer\n",
    "\n",
    "https://github.com/ccs-amsterdam/embeddingvectorizer\n",
    "pip install embeddingvectorizer\n",
    "    \n",
    "## download a pre-trained embedding model \n",
    "\n",
    "\n",
    "### Note: if you want to work with a potentially better and (much) large embedding model trained on > 7 milion Dutch news articels (9.5GB):\n",
    "https://surfdrive.surf.nl/files/index.php/s/S0tx1YNF7r0W9Lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embeddingvectorizer\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('dataset_vermeer.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split (df['text'], df['topic'], test_size = 0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_mdl\n",
    "model = gensim.models.Word2Vec.load('../tmpanne/fullsample/w2v_model_nr_7_window_10_size_300_negsample_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# get embedding model in right format (vector array for each dictionary word)\n",
    "embedding_mdl = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "# count vectorizer\n",
    "embedding_vect_count = embeddingvectorizer.EmbeddingCountVectorizer(embedding_mdl, 'mean')\n",
    "\n",
    "# tfidf\n",
    "embedding_vect_tfidf = embeddingvectorizer.EmbeddingTfidfVectorizer(embedding_mdl, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "clf = LogisticRegressionCV()\n",
    "pipe = make_pipeline(embedding_vect_count, clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set paths and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AEM = '../tmpanne/fullsample/w2v_model_nr_7_window_10_size_300_negsample_15'\n",
    "PATH_TO_DATA = \"../tmpanne/AEM_data/dataset_vermeer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KeyedVectors.load_word2vec_format(AEM)\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationWarning",
     "evalue": "Deprecated. Use model.wv.save_word2vec_format instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecationWarning\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-54a08beb8e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../tmpanne/fullsample/AEM300.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msave_word2vec_format\u001b[0;34m(self, fname, fvocab, binary)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \"\"\"\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deprecated. Use model.wv.save_word2vec_format instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecationWarning\u001b[0m: Deprecated. Use model.wv.save_word2vec_format instead."
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(AEM)\n",
    "model.wv.save_word2vec_format('../tmpanne/fullsample/AEM300.txt', binary=False)\n",
    "\n",
    "AEM = '../tmpanne/fullsample/AEM300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeY(Y):\n",
    "    '''create one-hot (dummies) for output, encode class values as integers\n",
    "    '''\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    dummy_y = tf.keras.utils.to_categorical(encoded_Y)\n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split([t.translate(str.maketrans('', '', string.punctuation)) for t in df['text']], encodeY(df['topic'].map(int)), test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(X_train)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in X_train])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "#ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(X_test)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1055100it [02:00, 8720.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1055100 word vectors.\n",
      "Should be 1055100 vectors with 300 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(AEM) as f:\n",
    "    numberofwordvectors, dimensions = [int(e) for e in next(f).split()]\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "       # word = values[0]\n",
    "       # coefs = np.asarray(values[1:], dtype='float32')\n",
    "      #  embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print('Should be {} vectors with {} dimensions'.format(numberofwordvectors, dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = [e for e in ][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum([np.isnan(vector.any()) for vector in embeddings_index.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    words_not_found = 0 \n",
    "    total_words = 0\n",
    "    DEBUG_lijstmetwoorden = []\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in tqdm(vocab.items()):\n",
    "        e = embedding.get(word, None)\n",
    "        if e is not None:   # if we do not find the word, we do not want to replace anything but leave the zero's\n",
    "            weight_matrix[i] = e\n",
    "            total_words+=1\n",
    "        else:\n",
    "            words_not_found+=1\n",
    "            DEBUG_lijstmetwoorden.append(word)\n",
    "    print('Weight matrix created. For {} out of {} words, we did not have any embedding.'.format(words_not_found, total_words))\n",
    "    return DEBUG_lijstmetwoorden, weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91757/91757 [00:00<00:00, 211077.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix created. For 22674 out of 69083 words, we did not have any embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missingwords, embedding_vectors = get_weight_matrix(embeddings_index, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 4.85866785, -2.4100337 ,  0.33942223, ...,  2.51087785,\n",
       "         1.81039345,  2.77537751],\n",
       "       [-3.1060853 ,  3.63483071,  1.44617176, ...,  1.62585211,\n",
       "        -4.65819836,  1.23772573],\n",
       "       ...,\n",
       "       [-0.82925767,  1.02513301, -0.58831489, ..., -0.06041323,\n",
       "        -0.35370061, -0.27203554],\n",
       "       [-0.73798001, -0.20098439, -1.77954566, ...,  0.07529053,\n",
       "        -1.03519547, -1.42382717],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91758, 2788)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vectors), len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(tokenizer.word_index)+1, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 10:22:17.244640 140267654510336 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25335, 300)        27527400  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 25331, 128)        192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 12665, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1621120)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 6484484   \n",
      "=================================================================\n",
      "Total params: 34,204,012\n",
      "Trainable params: 6,676,612\n",
      "Non-trainable params: 27,527,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25335, 300)        27527400  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 25332, 128)        153728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1583, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 202624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                12968000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 40,649,388\n",
      "Trainable params: 13,121,988\n",
      "Non-trainable params: 27,527,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# alternatief model\n",
    "\n",
    "numberoflabels = 4\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(128, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=numberoflabels, activation='softmax'))   # voor twee categorien sigmoid, voor 1 tanh\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2588 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "2588/2588 [==============================] - 288s 111ms/sample - loss: 3.0279 - acc: 0.5564 - val_loss: 0.9084 - val_acc: 0.6850\n",
      "Epoch 2/3\n",
      "2588/2588 [==============================] - 291s 113ms/sample - loss: 0.6169 - acc: 0.7732 - val_loss: 0.5981 - val_acc: 0.7950\n",
      "Epoch 3/3\n",
      "2588/2588 [==============================] - 290s 112ms/sample - loss: 0.3596 - acc: 0.8845 - val_loss: 0.5515 - val_acc: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f921157a630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATION_SIZE=200\n",
    "\n",
    "model.fit(Xtrain[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=3, verbose=True,\n",
    "          validation_data=(Xtrain[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 23s 33ms/sample - loss: 1.3055 - acc: 0.7779\n",
      "Test Accuracy: 77.793694\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(Xtest, y_test, verbose=True)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compile network\n",
    "# fit network\n",
    "model.fit(Xtrain, y_train, epochs=3, verbose=True)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index['man'] - embeddings_index['vrouw']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

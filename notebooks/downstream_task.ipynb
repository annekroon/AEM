{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised machinelearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_models = '/Users/anne/repos/embedding_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data ='../data/'\n",
    "df = pd.read_pickle(path_to_data + \"data_geannoteerd.pkl\")\n",
    "data = df['text']\n",
    "labels = df['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from /Users/anne/repos/embedding_models/w2v_320d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading aem_320d model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loaded /Users/anne/repos/embedding_models/w2v_320d\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading aem_320d model\")\n",
    "\n",
    "mod = gensim.models.Word2Vec.load(path_to_models + 'w2v_320d')\n",
    "aem_320d = dict(zip(mod.wv.index2word, mod.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from /Users/anne/repos/embedding_models/w2v_300d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading aem_300d model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loaded /Users/anne/repos/embedding_models/w2v_300d\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading aem_300d model\")\n",
    "\n",
    "mod = gensim.models.Word2Vec.load(path_to_models + 'w2v_300d')\n",
    "aem_300d = dict(zip(mod.wv.index2word, mod.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from /Users/anne/repos/embedding_models/wiki.nl.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading wiki model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loaded (871022, 300) matrix from /Users/anne/repos/embedding_models/wiki.nl.vec\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading wiki model\")\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_models + 'wiki.nl.vec')\n",
    "wiki = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from /Users/anne/repos/embedding_models/cow-big.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading cow model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loaded (3110718, 320) matrix from /Users/anne/repos/embedding_models/cow-big.txt\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading cow model\")\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_models + 'cow-big.txt', binary=False)\n",
    "cow = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline: \"Sequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit.\"\n",
    "\n",
    "https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_aem_320d_tfidf = Pipeline([(\"word2vec vectorize\"r, TfidfEmbeddingVectorizer(aem_320d))])\n",
    "etree_aem_320d_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(aem_320d))])\n",
    "\n",
    "etree_aem_300d_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(aem_300d))])\n",
    "etree_aem_300d_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(aem_300d))]) \n",
    "\n",
    "etree_wiki_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(wiki))])\n",
    "etree_wiki_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(wiki))]) \n",
    "\n",
    "etree_cow_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(cow))])\n",
    "etree_cow_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(cow))]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3486, 320)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([(\"word2vec vectorizer\", EmbeddingTfidfVectorizer(aem_320d))])\n",
    "d = p.fit_transform(data)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_320d_tfidf_svm = Pipeline([\n",
    "    (\"word2vec vectorizer\", EmbeddingTfidfVectorizer(aem_320d)),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[160,   4,  28,   0],\n",
       "       [ 23,  98,  81,  10],\n",
       "       [ 10,  11, 603,   5],\n",
       "       [  5,   7,  51,  55]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_aem_320d_tfidf_svm.fit(X_train, y_train)\n",
    "preds = pipe_aem_320d_tfidf_svm.predict(X_test)\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7958297132927888"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-c0f709eb8bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mauc\u001b[0;34m(x, y, reorder)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdiff\u001b[0;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnot_equal\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "sklearn.metrics.auc(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_aem_320d = etree_aem_320d_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_aem_300d = etree_aem_300d_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_wiki = etree_wiki_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_cow = etree_cow_tfidf.fit_transform([x.split() for x in data], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(fit_object, data, labels):\n",
    "    logger.info('{} x entries and {} y entries'.format(fit_object.shape[0], len(labels )))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fit_object, labels, test_size=0.2, shuffle = True, random_state=42)\n",
    "    fit_model =  SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000, random_state=42).fit(X_train, y_train)\n",
    "    test_pred = fit_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "    test_recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "    test_f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "    return {'accuracy':test_accuracy, 'precision':test_precision, 'recall':test_recall, 'f1':test_f1score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:3486 x entries and 3486 y entries\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "INFO:__main__:3486 x entries and 3486 y entries\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "INFO:__main__:3486 x entries and 3486 y entries\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "INFO:__main__:3486 x entries and 3486 y entries\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "evaluation_data = {}\n",
    "evaluation_data['aem_320d'] = [fit_score(fit_aem_320d, data, labels)]\n",
    "evaluation_data['aem_300d'] = [fit_score(fit_aem_300d, data, labels)]\n",
    "evaluation_data['wiki'] = [fit_score(fit_wiki, data, labels)]\n",
    "evaluation_data['cow'] = [fit_score(fit_cow, data, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:3486 x entries and 3486 y entries\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "fitted = vectorizer.fit_transform(data, labels)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "logger.info('{} x entries and {} y entries'.format(fitted.shape[0], len(labels )))\n",
    "X_train, X_test, y_train, y_test = train_test_split(fitted, labels, test_size=0.2, shuffle = True, random_state=42)\n",
    "fit_model =  SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "test_pred = fit_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_pred)\n",
    "test_precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "test_recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "test_f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "\n",
    "evaluation_data['normal_ml'] = ({'accuracy': test_accuracy, 'precision':test_precision, 'recall':test_recall, 'f1':test_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aem_320d': [{'accuracy': 0.8108882521489972,\n",
       "   'precision': 0.801774163636444,\n",
       "   'recall': 0.725057918202272,\n",
       "   'f1': 0.7535051927567186}],\n",
       " 'aem_300d': [{'accuracy': 0.8065902578796562,\n",
       "   'precision': 0.7951561035157512,\n",
       "   'recall': 0.7151567145637501,\n",
       "   'f1': 0.7444336887596226}],\n",
       " 'wiki': [{'accuracy': 0.8051575931232091,\n",
       "   'precision': 0.7832821081988672,\n",
       "   'recall': 0.7105611136416967,\n",
       "   'f1': 0.7386252243582465}],\n",
       " 'cow': [{'accuracy': 0.7636103151862464,\n",
       "   'precision': 0.7708249904652936,\n",
       "   'recall': 0.6341084724005134,\n",
       "   'f1': 0.6718788863903697}],\n",
       " 'normal_ml': {'accuracy': 0.7449856733524355,\n",
       "  'precision': 0.812801640926641,\n",
       "  'recall': 0.5865357502726132,\n",
       "  'f1': 0.634124448688369}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import numpy as np\n",
    "from scipy.sparse import spmatrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "class BaseEmbeddingVectorizerMixin:\n",
    "    \"\"\"\n",
    "    Base class for Embedding vectorizer\n",
    "    Note that we implement this as vectorizer rather than transformer because we need to access the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X: Iterable[str], y=None):\n",
    "        x = super().fit_transform(X, y).tocsr()\n",
    "        return np.array(list(self._transform(x)))\n",
    "    \n",
    "    def transform(self, X: Iterable[str], y=None):\n",
    "        x = super().transform(X, y).tocsr()\n",
    "        return np.array(list(self._transform(x)))\n",
    "\n",
    "    def _transform(self, x):\n",
    "        dim = len(next(iter(self.word2vec.values())))\n",
    "        voca = self._get_words()\n",
    "        for doc in range(len(x.indptr) - 1):\n",
    "            weights = x.data[x.indptr[doc]:x.indptr[doc + 1]]\n",
    "            words = x.indices[x.indptr[doc]:x.indptr[doc + 1]]\n",
    "\n",
    "            \n",
    "            vec = [self.word2vec[voca[w]] * np.array(weights[i])\n",
    "                   for i, w in enumerate(words) if voca[w] in self.word2vec]\n",
    "            yield np.mean(vec, axis=0) if vec else np.zeros(dim)\n",
    "\n",
    "    def _get_words(self):\n",
    "        # pretty ugly!\n",
    "        result = [None] * len(self.vocabulary_)\n",
    "        for w, i in self.vocabulary_.items():\n",
    "            result[i] = w\n",
    "        return result\n",
    "\n",
    "\n",
    "class EmbeddingCountVectorizer(BaseEmbeddingVectorizerMixin, CountVectorizer):\n",
    "    def __init__(self, word2vec, dim=320, **kargs):\n",
    "        # WvA: apparently we should enumerate all arguments rather than use **kargs,\n",
    "        # e.g. https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/feature_extraction/text.py#L1493-L1509\n",
    "        super().__init__(**kargs)\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "\n",
    "class EmbeddingTfidfVectorizer(BaseEmbeddingVectorizerMixin, TfidfVectorizer):\n",
    "    def __init__(self, word2vec, dim=320, **kargs):\n",
    "        # WvA: apparently we should enumerate all arguments rather than use **kargs,\n",
    "        # e.g. https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/feature_extraction/text.py#L1493-L1509\n",
    "        super().__init__(**kargs)\n",
    "        self.word2vec = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0304905 , -0.04684455, -0.03192578, ..., -0.11298548,\n",
       "         0.08396155,  0.04818919],\n",
       "       [ 0.02606081, -0.01315702, -0.05728958, ..., -0.01349195,\n",
       "         0.00125355,  0.02725781],\n",
       "       [ 0.01504622, -0.0007526 , -0.02787443, ..., -0.02110858,\n",
       "         0.02196479,  0.00411789],\n",
       "       [-0.01094059, -0.14049055, -0.36714485, ...,  0.07563913,\n",
       "         0.19668353,  0.1568886 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "texts = [\"dit is een text\", \"en dit is een kat\", \"en dit is een langere zin met heel veel woorden erin\", \"wie?\"]\n",
    "model = {\"dit\": [3, 0, 0], \"kat\": [0, 1, 0], \"woorden\": [0, 0, 1]}\n",
    "v = EmbeddingTfidfVectorizer(aem_320d, 3)\n",
    "v.fit_transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       3\n",
       "3       3\n",
       "4       1\n",
       "5       2\n",
       "6       4\n",
       "7       3\n",
       "8       3\n",
       "9       3\n",
       "11      1\n",
       "12      2\n",
       "13      2\n",
       "14      3\n",
       "15      2\n",
       "16      2\n",
       "17      3\n",
       "18      3\n",
       "19      2\n",
       "20      3\n",
       "21      3\n",
       "22      3\n",
       "23      2\n",
       "25      3\n",
       "26      3\n",
       "28      2\n",
       "29      3\n",
       "30      3\n",
       "31      4\n",
       "32      3\n",
       "       ..\n",
       "3181    3\n",
       "3182    3\n",
       "3183    3\n",
       "3184    3\n",
       "3185    3\n",
       "3186    3\n",
       "3187    3\n",
       "3188    2\n",
       "3189    3\n",
       "3190    3\n",
       "3191    3\n",
       "3192    3\n",
       "3193    3\n",
       "3194    2\n",
       "3195    3\n",
       "3196    3\n",
       "3197    3\n",
       "3198    3\n",
       "3199    4\n",
       "3200    3\n",
       "3201    1\n",
       "3202    2\n",
       "3203    3\n",
       "3204    3\n",
       "3205    1\n",
       "3206    3\n",
       "3207    3\n",
       "3208    3\n",
       "3209    3\n",
       "3210    3\n",
       "Name: topic, Length: 3486, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 3486\n"
     ]
    }
   ],
   "source": [
    "path_to_data ='../data/'\n",
    "df = pd.read_pickle(path_to_data + \"data_geannoteerd.pkl\")\n",
    "\n",
    "X, y = [], []\n",
    "X, y = np.array(df['text']), np.array(df['topic'])\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model            score\n",
      "-------------  -------\n",
      "w2v_tfidf       0.6520\n",
      "w2v             0.6486\n",
      "mult_nb_tfidf   0.5591\n",
      "mult_nb         0.5281\n",
      "bern_nb         0.4424\n",
      "bern_nb_tfidf   0.4424\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.model_selection._fit_and_score` not found.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "sklearn.model_selection._fit_and_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x128de7208>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=[name for name, _ in scores], y=[score for _, score in scores])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

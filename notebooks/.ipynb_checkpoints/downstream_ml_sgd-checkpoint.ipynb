{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "import embeddingvectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data ='../data/'\n",
    "df = pd.read_pickle(path_to_data + \"data_geannoteerd.pkl\")\n",
    "data = df['text']\n",
    "labels = df['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/anne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =df['text']\n",
    "df['text_stop']= df['text'].str.lower()\n",
    "stop = set(stopwords.words('dutch'))\n",
    "df['text_stop']= df['text_stop'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_stop']= df['text_stop'].apply(lambda x:[ item for item in x if item not in stop ])\n",
    "df['text_stop']= df['text_stop'].apply(lambda x:' '.join(x))\n",
    "text_stop = df['text_stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lead'] = df['text_stop']\n",
    "def proc(s):\n",
    "    l = s.split()\n",
    "    return ' '.join(l[:75])\n",
    "df['text_lead'] =[ proc (s) for s in df['text_lead'].values.tolist() ]\n",
    "text_lead =df['text_lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcolumns = {'text': text, 'text_stop': text_stop, 'text_lead': text_lead}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2788, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "gridsearchresults = pd.DataFrame()\n",
    "DV = 'topic'\n",
    "y= df[DV].as_matrix()\n",
    "X= df[textcolumns]\n",
    "X_train , X_test , y_train , y_test = train_test_split (X, y, test_size =0.2 , random_state =42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline.fit(X_train['text'], y_train)\n",
    "preds = clf_pipeline.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_df (precision, recall, fscore,  report_name):\n",
    "    report_data =[]\n",
    "    row = {}\n",
    "    row['classifier']= report_name\n",
    "    #row['class']= row_data [0]\n",
    "    row['precision']= float( precision)\n",
    "    row['recall']= float(recall)\n",
    "    row['f1_score']= float(fscore)\n",
    "    #row['support']= support\n",
    "    report_data.append(row )\n",
    "    print(report_data)\n",
    "    dataframe =pd.DataFrame.from_dict(report_data )\n",
    "    print(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applybestparams(classifier , X_train , X_name , y_train , X_test , y_test , gridsearchresults ):\n",
    "    bestparams = gridsearchresults [( gridsearchresults.classifier == classifier ) & ( gridsearchresults.textcolumn == X_name )]\n",
    "    print(bestparams)\n",
    "    \n",
    "    bp = bestparams.iloc[0].to_dict()\n",
    "    \n",
    "    clf_pipeline = Pipeline([('vect', CountVectorizer() ),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter'], random_state=42))\n",
    "    ])\n",
    "    \n",
    "    text_clf = clf_pipeline.fit (X_train[X_name], y_train )   \n",
    "    predicted = text_clf.predict(X_test [ X_name ])\n",
    "    \n",
    "    \n",
    "    precision, recall, fscore, support= score(y_test, predicted, average='macro')\n",
    "    \n",
    "    return {'accuracy': test_accuracy, 'precision': test_precision, 'recall':test_recall, 'f1':test_f1score}\n",
    "    res_apply = classification_report_df (precision, recall, fscore,  'SGD_' + X_name )\n",
    "    #joblib.dump (text_clf, 'PassiveAggressive_' + X_name + 'string.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start GridSearch for model {} ...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "INFO:root:\n",
      "Loading gensim model\n",
      "INFO:gensim.utils:loading Word2Vec object from /Users/anne/repos/embedding_models/test/w2v_model_nr_0_window_5_size_100_negsample_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.833):\n",
      "\n",
      "Best parameters: {'clf__alpha': 1e-05, 'clf__max_iter': 30, 'clf__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loaded /Users/anne/repos/embedding_models/test/w2v_model_nr_0_window_5_size_100_negsample_5\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "INFO:root:loaded gensim model nr 1, named: w2v_model_nr_0_window_5_size_100_negsample_5\n",
      "INFO:root:>>>> Retrieving best parameter settings for the model w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "INFO:root:Apply best parameter setting for the model w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "INFO:root:fitted...w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "INFO:root:predicted...w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Save results\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-609e969b2460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m#print(\"this is the result:\", result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[]]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "basepath = '/Users/anne/repos/embedding_models/test/'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=42)\n",
    "\n",
    "\n",
    "class word2vec_analyzer():\n",
    "    '''This class tests the efficacy of Word2Vec models in downstream tasks.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nmodel = 0\n",
    "        self.vectorizer = 'Tfidf'\n",
    "        \n",
    "       # logger.info(\"Created analyzer with {} combinations for crime and {} combinations for low life\".format(\n",
    "       #     len(self.combinations_crime), len(self.combinations_low)))\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        '''yields a dict with one item. key is the filename, value the gensim model'''\n",
    "        \n",
    "        filenames = [e for e in os.listdir(basepath) if not e.startswith('.')]\n",
    "\n",
    "        for fname in filenames:\n",
    "            model = {}\n",
    "            path = os.path.join(basepath, fname)\n",
    "            logger.info(\"\\nLoading gensim model\")\n",
    "            mod = gensim.models.Word2Vec.load(path)\n",
    "            model['gensimmodel'] = dict(zip(mod.wv.index2word, mod.wv.syn0))\n",
    "            model['filename'] = fname\n",
    "            #splitResult = fname.split( \"_\" ) #split on scores\n",
    "            self.nmodel +=1\n",
    "            logger.info(\"loaded gensim model nr {}, named: {}\".format(self.nmodel, model['filename']))\n",
    "            yield model\n",
    "        \n",
    "    def get_best_parameters(self):\n",
    "        \n",
    "        if self.vectorizer == 'Tfidf':\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "            ])\n",
    "            \n",
    "        param_grid =  {'clf__max_iter': (20, 30) , 'clf__alpha': (0.00001, 0.000001), 'clf__penalty': ('l2', 'elasticnet')}\n",
    "\n",
    "        search = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "        logger.info(\"Start GridSearch ...\")\n",
    "\n",
    "        search.fit(X_train, y_train )   \n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print()\n",
    "        print(\"Best parameters:\", search.best_params_)\n",
    "\n",
    "        return search.best_params_\n",
    "    \n",
    "    def apply_bestparameters_baseline(self, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "        if self.vectorizer == \"Tfidf\":\n",
    "            \n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter'] )),\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter'] )),\n",
    "            ])\n",
    "\n",
    "        clf = pipeline.fit(X_train, y_train)   \n",
    "        logger.info(\"fitted baseline model...\")\n",
    "\n",
    "        predicted = clf.predict(X_test)\n",
    "        logger.info(\"predicted baseline model... \")\n",
    "\n",
    "        precision, recall, fscore, support= score(y_test, predicted, average='macro')\n",
    "        \n",
    "        results.append({'precision': precision, \n",
    "        'recall': recall, \n",
    "        'f1': fscore, \n",
    "        'classifier' : \"SGD\", \n",
    "        'penalty' : bp['clf__penalty'], \n",
    "        'alpha' : bp['clf__alpha'], \n",
    "        'max_iter' : bp['clf__max_iter'] , \n",
    "        'model' : model['filename']})\n",
    "\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def apply_bestparameters_w2v(self, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "        results = []\n",
    "        bp = self.get_best_parameters()\n",
    "        \n",
    "        for model in self.get_w2v_model():\n",
    "            \n",
    "            logger.info(\">>>> Retrieving best parameter settings for the model {} ...\".format(model['filename']))\n",
    "            if self.vectorizer == \"Tfidf\":\n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "\n",
    "                pipeline = Pipeline([(\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')),\n",
    "                                     (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "                    ])\n",
    "            else:\n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "                pipeline = Pipeline([\n",
    "                    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')),\n",
    "                    (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "                ])\n",
    "\n",
    "\n",
    "            clf = pipeline.fit(X_train, y_train)   \n",
    "            logger.info(\"fitted...{} ...\".format(model['filename']))\n",
    "\n",
    "            predicted = clf.predict(X_test)\n",
    "            logger.info(\"predicted...{} ...\".format(model['filename']))\n",
    "\n",
    "            precision, recall, fscore, support= score(y_test, predicted, average='macro')\n",
    "\n",
    "            results.append({'precision': precision, \n",
    "                    'recall': recall, \n",
    "                    'f1': fscore, \n",
    "                    'classifier' : \"SGD\", \n",
    "                    'penalty' : bp['clf__penalty'], \n",
    "                    'alpha' : bp['clf__alpha'], \n",
    "                    'max_iter' : bp['clf__max_iter'] , \n",
    "                    'model' : model['filename']})\n",
    "            \n",
    "            return results\n",
    "        \n",
    "    def get_final_results(self):\n",
    "        results_baseline = self.apply_bestparameters_baseline()\n",
    "        results_w2v = self.apply_bestparameters_w2v()\n",
    "        return results_baseline + results_w2v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    myanalyzer = word2vec_analyzer()\n",
    "    my_results = myanalyzer.apply_bestparameters_w2v()\n",
    "    \n",
    "    print(\"\\n\\n\\nSave results\\n\\n\\n\")\n",
    "    with open('output_my_results.json',mode='w') as fo:\n",
    "        fo.write('[')\n",
    "        \n",
    "        for result in my_results:\n",
    "            #print(\"this is the result:\", result)\n",
    "            fo.write(json.dumps(result))\n",
    "            fo.write(',\\n')\n",
    "        fo.write('[]]')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(my_results)\n",
    "    print('Created dataframe')\n",
    "    # print(df)\n",
    "    df.to_csv('w2v_evaluation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Loading gensim model\n",
      "INFO:gensim.utils:loading Word2Vec object from /Users/anne/repos/embedding_models/test/w2v_model_nr_0_window_5_size_100_negsample_5\n",
      "INFO:gensim.utils:loaded /Users/anne/repos/embedding_models/test/w2v_model_nr_0_window_5_size_100_negsample_5\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "INFO:root:loaded gensim model nr 1, named: w2v_model_nr_0_window_5_size_100_negsample_5\n",
      "INFO:root:>>>> Retrieving best parameter settings for the model w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "INFO:root:Starting gridsearch to optimize parameter setting for model w2v_model_nr_0_window_5_size_100_negsample_5 using TfidfVectorizer ...\n",
      "INFO:root:Start GridSearch for model w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "INFO:root:Apply best parameter setting for the model w2v_model_nr_0_window_5_size_100_negsample_5 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.559):\n",
      "\n",
      "Best parameters: {'clf__alpha': 1e-05, 'clf__max_iter': 30, 'clf__penalty': 'elasticnet'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "INFO:root:fitted...w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "INFO:root:predicted...w2v_model_nr_0_window_5_size_100_negsample_5 ...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3d7c371388cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyanalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyanalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_bestparameters_w2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b501de65f0bf>\u001b[0m in \u001b[0;36mapply_bestparameters_w2v\u001b[0;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'alpha'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf__alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'max_iter'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf__max_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;34m'vectorizer'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 'model' : model['filename']})\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vect' is not defined"
     ]
    }
   ],
   "source": [
    "myanalyzer = word2vec_analyzer()\n",
    "results = myanalyzer.apply_bestparameters_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/Users/anne/repos/embedding_models/test/'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=42)\n",
    "\n",
    "\n",
    "class word2vec_analyzer():\n",
    "    '''This class tests the efficacy of Word2Vec models in downstream tasks.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nmodel = 0\n",
    "        self.nbasemodel = 0\n",
    "        self.vectorizer = 'Tfidf'\n",
    "        \n",
    "       # logger.info(\"Created analyzer with {} combinations for crime and {} combinations for low life\".format(\n",
    "       #     len(self.combinations_crime), len(self.combinations_low)))\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        '''yields a dict with one item. key is the filename, value the gensim model'''\n",
    "        \n",
    "        filenames = [e for e in os.listdir(basepath) if not e.startswith('.')]\n",
    "\n",
    "        for fname in filenames:\n",
    "            model = {}\n",
    "            path = os.path.join(basepath, fname)\n",
    "            logger.info(\"\\nLoading gensim model\")\n",
    "            mod = gensim.models.Word2Vec.load(path)\n",
    "            model['gensimmodel'] = dict(zip(mod.wv.index2word, mod.wv.syn0))\n",
    "            model['filename'] = fname\n",
    "            #splitResult = fname.split( \"_\" ) #split on scores\n",
    "            self.nmodel +=1\n",
    "            logger.info(\"loaded gensim model nr {}, named: {}\".format(self.nmodel, model['filename']))\n",
    "            yield model\n",
    "        \n",
    "    def get_baseline_model(self):\n",
    "        '''yields a dict with one item. key is the filename, value the gensim model'''\n",
    "        basemodel = {}\n",
    "        basemodel['baseline'] = \"baseline\"\n",
    "        self.nbasemodel +=1\n",
    "        yield basemodel\n",
    "        \n",
    "    def get_best_parameters_basemodel(self, model):\n",
    "        \n",
    "        if self.vectorizer == 'Tfidf':\n",
    "            pipeline = Pipeline([\n",
    "                    ('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "                ])\n",
    "\n",
    "\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                    ('vect', CountVectorizer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "                ])\n",
    "        \n",
    "        param_grid =  {'clf__max_iter': (20, 30) , 'clf__alpha': (0.00001, 0.000001), 'clf__penalty': ('l2', 'elasticnet')}\n",
    "\n",
    "        search = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "        logger.info(\"Start GridSearch for model {} ...\".format(model['filename']))\n",
    "\n",
    "        search.fit(X_train, y_train )   \n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print()\n",
    "        print(\"Best parameters:\", search.best_params_)\n",
    "\n",
    "        return search.best_params_\n",
    "\n",
    "\n",
    "        \n",
    "    def get_best_parameters_w2v(self, model):\n",
    "        \n",
    "        if self.vectorizer =='Tfidf':\n",
    "            logger.info(\"Starting gridsearch to optimize parameter setting for model {} using TfidfVectorizer ...\".format(model['filename']))\n",
    "\n",
    "            pipeline = Pipeline([(\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')),\n",
    "                                 (\"clf\", SGDClassifier(loss='hinge', tol=1e-4))\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            logger.info(\"Starting gridsearch to optimize parameter setting for model {} using CountVectorizer...\".format(model['filename']))\n",
    "            pipeline = Pipeline([(\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')),\n",
    "                             (\"clf\", SGDClassifier(loss='hinge', tol=1e-4))\n",
    "            ])\n",
    "\n",
    "\n",
    "        param_grid =  {'clf__max_iter': (20, 30) , 'clf__alpha': (0.00001, 0.000001), 'clf__penalty': ('l2', 'elasticnet')}\n",
    "\n",
    "\n",
    "        search = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "        logger.info(\"Start GridSearch for model {} ...\".format(model['filename']))\n",
    "\n",
    "        search.fit(X_train, y_train )   \n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print()\n",
    "        print(\"Best parameters:\", search.best_params_)\n",
    "\n",
    "        return search.best_params_\n",
    "    \n",
    "    \n",
    "    def apply_bestparameters_w2v(self, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "        results = []\n",
    "        \n",
    "        for model in self.get_w2v_model():\n",
    "            \n",
    "            logger.info(\">>>> Retrieving best parameter settings for the model {} ...\".format(model['filename']))\n",
    "            bp = self.get_best_parameters_w2v(model)\n",
    "\n",
    "            if self.vectorizer == \"Tfidf\":\n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "\n",
    "                pipeline = Pipeline([(\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')),\n",
    "                                     (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "                    ])\n",
    "\n",
    "            else:\n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "                pipeline = Pipeline([\n",
    "                    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')),\n",
    "                    (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "                ])\n",
    "\n",
    "                \n",
    "                clf = pipeline.fit(X_train, y_train)   \n",
    "                logger.info(\"fitted...{} ...\".format(model['filename']))\n",
    "\n",
    "                predicted = clf.predict(X_test)\n",
    "                logger.info(\"predicted...{} ...\".format(model['filename']))\n",
    "\n",
    "                precision, recall, fscore, support= score(y_test, predicted, average='macro')\n",
    "\n",
    "                results.append({'precision': precision, \n",
    "                        'recall': recall, \n",
    "                        'f1': fscore, \n",
    "                        'classifier' : \"SGD\", \n",
    "                        'penalty' : bp['clf__penalty'], \n",
    "                        'alpha' : bp['clf__alpha'], \n",
    "                        'max_iter' : bp['clf__max_iter'] , \n",
    "                        'vectorizer' : vect , \n",
    "                        'model' : model['filename']})\n",
    "\n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    myanalyzer = word2vec_analyzer()\n",
    "    my_results = myanalyzer.apply_bestparameters_w2v()\n",
    "    \n",
    "    print(\"\\n\\n\\nSave results\\n\\n\\n\")\n",
    "    with open('output_my_results.json',mode='w') as fo:\n",
    "        fo.write('[')\n",
    "        \n",
    "        for result in my_results:\n",
    "            #print(\"this is the result:\", result)\n",
    "            fo.write(json.dumps(result))\n",
    "            fo.write(',\\n')\n",
    "        fo.write('[]]')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(gensimscore)\n",
    "    print('Created dataframe')\n",
    "    # print(df)\n",
    "    df.to_csv('w2v_evaluation.csv')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_parameters(model = None, Tfidf = False):\n",
    "    \n",
    "    if model == None:\n",
    "        if Tfidf == True:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "            ])\n",
    "\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4)),\n",
    "            ])\n",
    "\n",
    "   \n",
    "    if model != None:\n",
    "        if Tfidf == True:\n",
    "            pipeline = Pipeline([(\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')),\n",
    "                                 (\"clf\", SGDClassifier(loss='hinge', tol=1e-4))\n",
    "                ])\n",
    "\n",
    "        if Tfidf == False:\n",
    "            pipeline = Pipeline([(\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')),\n",
    "                             (\"clf\", SGDClassifier(loss='hinge', tol=1e-4))\n",
    "            ])\n",
    "   \n",
    "    param_grid =  {'clf__max_iter': (20, 1000, 5000) , 'clf__alpha': (0.00001, 0.000001), 'clf__penalty': ('l2', 'elasticnet')}\n",
    "\n",
    "    \n",
    "    search = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "    search.fit(X_train, y_train )   \n",
    "\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print()\n",
    "    print(\"Best parameters:\", search.best_params_)\n",
    "    \n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bestparameters(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, model = \"baseline\", vect = \"Tfidf\"):\n",
    "    \n",
    "   # bp = get_best_parameters()\n",
    "    bp = {'clf__alpha': 1e-06, 'clf__max_iter': 1000, 'clf__penalty': 'elasticnet'}\n",
    "    \n",
    "    if model == \"baseline\":\n",
    "        if vect == \"Tfidf\":\n",
    "            \n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter'] )),\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter'] )),\n",
    "            ])\n",
    "            \n",
    "            \n",
    "    if model != \"baseline\":\n",
    "        if vect == \"Tfidf\":\n",
    "            \n",
    "            pipeline = Pipeline([(\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')),\n",
    "                                 (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "                ])\n",
    "\n",
    "               \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')),\n",
    "                (\"clf\", SGDClassifier(loss='hinge', tol=1e-4, penalty=bp['clf__penalty'], alpha=bp['clf__alpha'], max_iter=bp['clf__max_iter']))\n",
    "            ])\n",
    "   \n",
    "\n",
    "    clf = pipeline.fit(X_train, y_train)   \n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    precision, recall, fscore, support= score(y_test, predicted, average='macro')\n",
    "    \n",
    "    return {'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1': fscore, \n",
    "            'classifier' : \"SGD\", \n",
    "            'penalty' : bp['clf__penalty'], \n",
    "            'alpha' : bp['clf__alpha'], \n",
    "            'max_iter' : bp['clf__max_iter'] , \n",
    "            'vectorizer' : vect , \n",
    "            'model' : model}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bestparameters(vect = \"Tfidf\", model = \"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=0)\n",
    "get_best_parameters(Tfidf = False, model = aem_320d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_parameters(Tfidf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    res_apply = applybestparams(model, X_train, textcol, y_train, X_test, y_test, gridsearchresults)\n",
    "    print(\"res_apply\", res_apply)\n",
    "    results = results.append(res_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading aem_320d model\")\n",
    "\n",
    "mod = gensim.models.Word2Vec.load(path_to_models + 'w2v_320d')\n",
    "aem_320d = dict(zip(mod.wv.index2word, mod.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading aem_300d model\")\n",
    "\n",
    "mod = gensim.models.Word2Vec.load(path_to_models + 'AEM_corpus/w2v_320d_AEM_corpus_2000-01-01_2017-12-31')\n",
    "aem_300d = dict(zip(mod.wv.index2word, mod.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading wiki model\")\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_models + 'wiki.nl.vec')\n",
    "wiki = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading cow model\")\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_models + 'cow-big.txt', binary=False)\n",
    "cow = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline: \"Sequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit.\"\n",
    "\n",
    "https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_aem_320d_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(aem_320d))])\n",
    "etree_aem_320d_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(aem_320d))])\n",
    "\n",
    "etree_aem_300d_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(aem_300d))])\n",
    "etree_aem_300d_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(aem_300d))]) \n",
    "\n",
    "etree_wiki_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(wiki))])\n",
    "etree_wiki_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(wiki))]) \n",
    "\n",
    "etree_cow_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(cow))])\n",
    "etree_cow_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(cow))]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_aem_320d_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(aem_320d))])\n",
    "etree_aem_320d_mean = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(aem_320d))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline([(\"word2vec vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(aem_320d, 'mean'))])\n",
    "d = p.fit_transform(data)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_320d = Pipeline([\n",
    "    (\"word2vec vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(aem_320d, 'mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline = Pipeline([('vect', CountVectorizer() ),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(tol=1e-3)),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline.fit(X_train, y_train)\n",
    "preds = clf_pipeline.predict(X_test)\n",
    "#confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True, max_iter=10000, tol=1e-5, random_state=0)\n",
    "\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "#digits = datasets.load_digits()\n",
    "X_digits = X_train\n",
    "y_digits = y_train\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 20, 30, 40, 50, 64],\n",
    "    'logistic__alpha': np.logspace(-4, 4, 5),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=5)\n",
    "\n",
    "search.fit(X_digits, y_digits)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_digits)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(aem_320d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_320d_tfidf_svm = Pipeline([\n",
    "    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(aem_320d, operator='mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['imp__strategy'] = ['mean', 'median', 'most_frequent']\n",
    "parameters['feat_select__k'] = [5, 10]\n",
    "\n",
    "CV = GridSearchCV(pipeline, parameters, scoring = 'mean_absolute_error', n_jobs= 1)\n",
    "CV.fit(x_train_cont, y_train)   \n",
    "\n",
    "print('Best score and parameter combination = ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_300d_count_svm = Pipeline([\n",
    "    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(aem_300d, operator='mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_320d_tfidf_svm = Pipeline([\n",
    "    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(aem_320d, operator='mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_normal_svm = Pipeline([\n",
    "    (\"vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(aem_320d, operator='mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_300d_tfidf_svm = Pipeline([\n",
    "    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(aem_300d, operator='mean')),\n",
    "    (\"svm\", SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test:\n",
    "    try:\n",
    "        i = float(i)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_320d_tfidf_svm.fit(X_train, y_train)\n",
    "preds = pipe_aem_320d_tfidf_svm.predict(X_test)\n",
    "\n",
    "#confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline.fit(X_train, y_train)\n",
    "preds = clf_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_300d_tfidf_svm.fit(X_train, y_train)\n",
    "preds = pipe_aem_300d_tfidf_svm.predict(X_test)\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_aem_300d_tfidf_svm.fit(X_train, y_train)\n",
    "preds = pipe_aem_300d_tfidf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.auc(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_aem_320d = etree_aem_320d_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_aem_300d = etree_aem_300d_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_wiki = etree_wiki_tfidf.fit_transform([x.split() for x in data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_cow = etree_cow_tfidf.fit_transform([x.split() for x in data], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(fit_object, data, labels):\n",
    "    logger.info('{} x entries and {} y entries'.format(fit_object.shape[0], len(labels )))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fit_object, labels, test_size=0.2, shuffle = True, random_state=42)\n",
    "    fit_model =  SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000, random_state=42).fit(X_train, y_train)\n",
    "    test_pred = fit_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "    test_recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "    test_f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "    return {'accuracy':test_accuracy, 'precision':test_precision, 'recall':test_recall, 'f1':test_f1score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = {}\n",
    "evaluation_data['aem_320d'] = [fit_score(fit_aem_320d, data, labels)]\n",
    "evaluation_data['aem_300d'] = [fit_score(fit_aem_300d, data, labels)]\n",
    "evaluation_data['wiki'] = [fit_score(fit_wiki, data, labels)]\n",
    "evaluation_data['cow'] = [fit_score(fit_cow, data, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "fitted = vectorizer.fit_transform(data, labels)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "logger.info('{} x entries and {} y entries'.format(fitted.shape[0], len(labels )))\n",
    "X_train, X_test, y_train, y_test = train_test_split(fitted, labels, test_size=0.2, shuffle = True, random_state=42)\n",
    "fit_model =  SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "test_pred = fit_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_pred)\n",
    "test_precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "test_recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "test_f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "\n",
    "evaluation_data['normal_ml'] = ({'accuracy': test_accuracy, 'precision':test_precision, 'recall':test_recall, 'f1':test_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import numpy as np\n",
    "from scipy.sparse import spmatrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "class BaseEmbeddingVectorizerMixin:\n",
    "    \"\"\"\n",
    "    Base class for Embedding vectorizer\n",
    "    Note that we implement this as vectorizer rather than transformer because we need to access the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X: Iterable[str], y=None):\n",
    "        x = super().fit_transform(X, y).tocsr()\n",
    "        return np.array(list(self._transform(x)))\n",
    "    \n",
    "    def transform(self, X: Iterable[str], y=None):\n",
    "        x = super().transform(X, y).tocsr()\n",
    "        return np.array(list(self._transform(x)))\n",
    "\n",
    "    def _transform(self, x):\n",
    "        dim = len(next(iter(self.word2vec.values())))\n",
    "        voca = self._get_words()\n",
    "        for doc in range(len(x.indptr) - 1):\n",
    "            weights = x.data[x.indptr[doc]:x.indptr[doc + 1]]\n",
    "            words = x.indices[x.indptr[doc]:x.indptr[doc + 1]]\n",
    "\n",
    "            \n",
    "            vec = [self.word2vec[voca[w]] * np.array(weights[i])\n",
    "                   for i, w in enumerate(words) if voca[w] in self.word2vec]\n",
    "            yield np.mean(vec, axis=0) if vec else np.zeros(dim)\n",
    "\n",
    "    def _get_words(self):\n",
    "        # pretty ugly!\n",
    "        result = [None] * len(self.vocabulary_)\n",
    "        for w, i in self.vocabulary_.items():\n",
    "            result[i] = w\n",
    "        return result\n",
    "\n",
    "\n",
    "class EmbeddingCountVectorizer(BaseEmbeddingVectorizerMixin, CountVectorizer):\n",
    "    def __init__(self, word2vec, dim=320, **kargs):\n",
    "        # WvA: apparently we should enumerate all arguments rather than use **kargs,\n",
    "        # e.g. https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/feature_extraction/text.py#L1493-L1509\n",
    "        super().__init__(**kargs)\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "\n",
    "class EmbeddingTfidfVectorizer(BaseEmbeddingVectorizerMixin, TfidfVectorizer):\n",
    "    def __init__(self, word2vec, dim=320, **kargs):\n",
    "        # WvA: apparently we should enumerate all arguments rather than use **kargs,\n",
    "        # e.g. https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/feature_extraction/text.py#L1493-L1509\n",
    "        super().__init__(**kargs)\n",
    "        self.word2vec = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [\"dit is een text\", \"en dit is een kat\", \"en dit is een langere zin met heel veel woorden erin\", \"wie?\"]\n",
    "model = {\"dit\": [3, 0, 0], \"kat\": [0, 1, 0], \"woorden\": [0, 0, 1]}\n",
    "v = EmbeddingTfidfVectorizer(aem_320d, 3)\n",
    "v.fit_transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data ='../data/'\n",
    "df = pd.read_pickle(path_to_data + \"data_geannoteerd.pkl\")\n",
    "\n",
    "X, y = [], []\n",
    "X, y = np.array(df['text']), np.array(df['topic'])\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "sklearn.model_selection._fit_and_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=[name for name, _ in scores], y=[score for _, score in scores])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "import embeddingvectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data ='../data/'\n",
    "df = pd.read_pickle(path_to_data + \"data_geannoteerd.pkl\")\n",
    "data = df['text']\n",
    "labels = df['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/home/anne/tmpanne/fullsample/'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.02, random_state=42)\n",
    "\n",
    "class word2vec_analyzer():\n",
    "    '''This class tests the efficacy of Word2Vec models in downstream tasks.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nmodel = 0\n",
    "        self.vectorizer = 'Tfidf'\n",
    "        self.param_grid = {'clf__penalty': ('l2', 'elasticnet') }\n",
    "        \n",
    "       # logger.info(\"Created analyzer with {} combinations for crime and {} combinations for low life\".format(\n",
    "       #     len(self.combinations_crime), len(self.combinations_low)))\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        '''yields a dict with one item. key is the filename, value the gensim model'''\n",
    "        \n",
    "        filenames = [e for e in os.listdir(basepath) if not e.startswith('.')]\n",
    "\n",
    "        for fname in filenames:\n",
    "            model = {}\n",
    "            path = os.path.join(basepath, fname)\n",
    "            logger.info(\"\\nLoading gensim model\")\n",
    "            mod = gensim.models.Word2Vec.load(path)\n",
    "            model['gensimmodel'] = dict(zip(mod.wv.index2word, mod.wv.syn0))\n",
    "            model['filename'] = fname\n",
    "            #splitResult = fname.split( \"_\" ) #split on scores\n",
    "            self.nmodel +=1\n",
    "            logger.info(\"loaded gensim model nr {}, named: {}\".format(self.nmodel, model['filename']))\n",
    "            yield model\n",
    "        \n",
    "    def get_best_parameters(self):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        if self.vectorizer == 'Tfidf':\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42)),\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42)),\n",
    "            ])\n",
    "            \n",
    "        param_grid = self.param_grid \n",
    "        search = GridSearchCV(pipeline, param_grid, iid=False, cv=5)\n",
    "        logger.info(\"Start GridSearch ...\")\n",
    "\n",
    "        search.fit(X_train, y_train )   \n",
    "      \n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print()\n",
    "        print(\"Best parameters:\", search.best_params_)\n",
    "        \n",
    "\n",
    "        return search.best_params_ \n",
    "    \n",
    "  \n",
    "    def apply_bestparameters_w2v(self, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n",
    "        \n",
    "        logger.info(\"retrieved the best parameter settings: {}...  \".format(bp))\n",
    "        logger.info(\"\\n\\n\\nthese are the results of the baseline model with hyperparameter optimalization: {}\".format(results_baseline))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        bp = self.get_best_parameters()\n",
    "        \n",
    "        if self.vectorizer == \"Tfidf\":\n",
    "            \n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', alpha=1e-6, tol=1e-4, max_iter=1000, random_state=42, penalty=bp['clf__penalty'])\n",
    "                 )])\n",
    "            \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', alpha=1e-6, tol=1e-4, max_iter=1000, random_state=42, penalty=bp['clf__penalty'])\n",
    "                 )])\n",
    "            \n",
    "        clf = pipeline.fit(X_train, y_train)   \n",
    "        logger.info(\"fitted baseline model.\")\n",
    "\n",
    "        test_pred = clf.predict(X_test)\n",
    "        logger.info(\"predicted baseline model.\")\n",
    "            \n",
    "        test_pred = search.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "        recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "        f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "        \n",
    "        results.append({'accuracy': accuracy, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1score': f1_score, \n",
    "            'classifier' : \"SGD\",\n",
    "            'model' : 'baseline'})\n",
    "        \n",
    "        \n",
    "        for model in self.get_w2v_model():\n",
    "            logger.info(\">>>> Retrieving best parameter settings for the model {} ...\".format(model['filename']))\n",
    "\n",
    "            \n",
    "            if self.vectorizer == \"Tfidf\":\n",
    "                \n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "                pipeline = Pipeline([\n",
    "                    (\"word2vec Tfidf vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(model['gensimmodel'], operator='mean')),\n",
    "                    (\"clf\", SGDClassifier(loss='hinge', alpha=1e-6, tol=1e-4, max_iter=1000, random_state=42, penalty=bp['clf__penalty']))\n",
    "                    ])\n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                logger.info(\"Apply best parameter setting for the model {} ...\".format(model['filename']))\n",
    "                pipeline = Pipeline([\n",
    "                    (\"word2vec Count vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(model['gensimmodel'], operator='mean')),\n",
    "                    (\"clf\", SGDClassifier(loss='hinge', alpha=1e-6, tol=1e-4, max_iter=1000, random_state=42, penalty=bp['clf__penalty']))\n",
    "                    ])\n",
    "                \n",
    "                \n",
    "            clf = pipeline.fit(X_train, y_train)   \n",
    "            logger.info(\"fitted...{} ...\".format(model['filename']))\n",
    "\n",
    "            test_pred = clf.predict(X_test)\n",
    "            logger.info(\"predicted...{} ...\".format(model['filename']))\n",
    "\n",
    "            accuracy = accuracy_score(y_test, test_pred)\n",
    "            precision = precision_score(y_test, test_pred, average = 'macro')\n",
    "            recall = recall_score(y_test, test_pred, average = 'macro')\n",
    "            f1score = f1_score(y_test, test_pred, average = 'macro')\n",
    "\n",
    "            results.append({'accuracy': accuracy, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1score': f1_score, \n",
    "            'classifier' : \"SGD\",\n",
    "            'model' : model['filename']})\n",
    "\n",
    "            print(\"these are the results of the w2v models:\", accuracy, precision, recall)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    myanalyzer = word2vec_analyzer()\n",
    "    my_results = myanalyzer.apply_bestparameters_w2v()\n",
    "    \n",
    "    print(\"\\n\\n\\nSave results\\n\\n\\n\")\n",
    "    with open('output_my_results.json',mode='w') as fo:\n",
    "        fo.write('[')\n",
    "        \n",
    "        for result in my_results:\n",
    "            #print(\"this is the result:\", result)\n",
    "            fo.write(json.dumps(result))\n",
    "            fo.write(',\\n')\n",
    "        fo.write('[]]')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(my_results)\n",
    "    print('Created dataframe')\n",
    "    # print(df)\n",
    "    df.to_csv('w2v_evaluation.csv')\n",
    "\n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
